MentorFlow: Evaluation-Driven RAG Framework ðŸš€
An Open-Source Methodology for Building Trustworthy AI Mentorship Systems.

ðŸŽ¯ Why This Project Exists
Most AI products fail because they lack reliability. This project isn't just another RAG wrapper; it is a live implementation of an Evaluation-Driven Learning Loop. It demonstrates how a Senior AI PM manages the gap between "it works on my machine" and "it's safe for enterprise users."

ðŸ›  Key Architecture (The "System Builder" Layer)
Versioned RAG Pipeline: Optimized retrieval logic moving from basic vector search to context-aware mentorship.

LLM-as-Judge Evaluation: Automated scoring rubrics that measure hallucination, relevance, and tone consistency.

Decision-Based Monitoring: Integrated tracking of model performance to drive Risk-Tiered Release decisions.

AI-Augmented Iteration: Built using an agentic workflow (Cursor/Copilot), showcasing 3x faster delivery from system design to functional MVP.

ðŸ“Š The Evidence (LTI Index Integration)
This repo serves as the technical validation for the LTI Index (LLM Trust & Iteration) methodology:

Phase 1: Defining Golden Datasets for mentorship edge cases.

Phase 2: Implementing the 0.94 Evaluation Learning Loop.

Phase 3: Transitioning from tactical evaluation to strategic AI Governance.

ðŸš€ Getting Started
Bash
# Clone the logic
git clone https://github.com/richardlee8433/mentorflow.git
# Explore the evaluation-driven approach
cd mentorflow && cat README.md
