{
  "id": "chapter1",
  "title": "Tokens, Embeddings, and Context Windows",
  "units": [
    {
      "id": "chapter1_unit1",
      "lesson": 1,
      "unit": "1.1",
      "title": "What is a token and why it matters",
      "material": "Large language models do not read raw characters or words. They read tokens, which are small pieces of text such as subwords, characters, or punctuation. Every prompt and response is converted into tokens. Costs, latency, and context limits are all calculated in tokens, not words. As an AI PM, you must think in tokens when designing prompts, sizing features, and estimating cost.",
      "question": "Why do AI product managers need to think in tokens instead of words?",
      "key_points": [
        "Model cost is charged per token",
        "Latency depends on the number of tokens processed",
        "Context window limits are defined in tokens",
        "Prompt and response length must be planned in tokens"
      ],
      "min_points": 1,
      "next_unit": "chapter1_unit2"
    },
    {
      "id": "chapter1_unit2",
      "lesson": 1,
      "unit": "1.2",
      "title": "How tokenization works in practice",
      "material": "Tokenization is the process of breaking text into tokens using a specific tokenizer (such as cl100k_base for many OpenAI models). The same sentence can have a different number of tokens depending on the tokenizer and language. Short words, punctuation, and even whitespace can become separate tokens. For planning capacity or cost, you approximate token counts using tools or rules of thumb, such as 3–4 tokens per English word.",
      "question": "When estimating usage for a feature, what is one practical way to approximate token counts?",
      "key_points": [
        "Use an approximate rule such as 3–4 tokens per English word",
        "Use a tokenizer tool or API to measure sample text",
        "Base estimates on realistic sample prompts and responses"
      ],
      "min_points": 1,
      "next_unit": "chapter1_unit3"
    },
    {
      "id": "chapter1_unit3",
      "lesson": 1,
      "unit": "1.3",
      "title": "Embeddings as vector representations",
      "material": "Embeddings turn text into numerical vectors so that similar meanings become close in vector space. They are used for search, recommendations, clustering, and Retrieval-Augmented Generation (RAG). For AI PMs, embeddings are a core tool for building semantic search over documents, matching user queries to knowledge, and measuring similarity between questions or feedback.",
      "question": "What is one common use case for text embeddings in AI products?",
      "key_points": [
        "Semantic search over documents or FAQs",
        "Recommendation or matching similar items",
        "Clustering or grouping similar content",
        "RAG systems that retrieve relevant context"
      ],
      "min_points": 1,
      "next_unit": "chapter1_unit4"
    },
    {
      "id": "chapter1_unit4",
      "lesson": 1,
      "unit": "1.4",
      "title": "Context windows and their limits",
      "material": "Each model has a maximum context window, which is the total number of tokens in the prompt plus the response. If you exceed this limit, the model will truncate or reject the request. Large context windows allow longer documents or conversations but also increase cost and latency. Designing a system means deciding what must be in-context now versus what can be summarized, stored externally, or omitted.",
      "question": "Why is understanding the model's context window important when designing a product feature?",
      "key_points": [
        "It prevents prompts from exceeding the maximum token limit",
        "It forces trade-offs between detail and cost",
        "It affects how much history or document content can be included",
        "It influences UX decisions like summarizing or trimming history"
      ],
      "min_points": 1,
      "next_unit": "chapter1_unit5"
    },
    {
      "id": "chapter1_unit5",
      "lesson": 1,
      "unit": "1.5",
      "title": "Budgeting and trade-offs in tokens",
      "material": "Every feature that calls an LLM has a token budget. As an AI PM, you often adjust three levers: how long user inputs can be, how detailed system prompts are, and how long responses should be. You also decide which metadata or context is worth spending tokens on. Good token budgeting balances accuracy, UX quality, and cost so that the product remains usable and economically viable.",
      "question": "Name one practical token trade-off you might make when designing an AI feature.",
      "key_points": [
        "Limiting maximum user input length",
        "Shortening or restructuring system prompts",
        "Capping response length or using short summaries",
        "Reducing the amount of retrieved context per call"
      ],
      "min_points": 1,
      "next_unit": null
    }
  ]
}
